"""Example of a figure 8 network with human-driven vehicles and RL training."""

import gym
import ray
from ray.rllib.agents.ppo import PPOTrainer
from ray.tune.registry import register_env
from flow.utils.registry import make_create_env

from flow.controllers import IDMController, StaticLaneChanger, ContinuousRouter, RLController
from flow.core.params import SumoParams, EnvParams, NetParams, VehicleParams, SumoCarFollowingParams
from flow.envs.ring.accel import ADDITIONAL_ENV_PARAMS
from flow.networks.figure_eight import ADDITIONAL_NET_PARAMS
from flow.envs.multiagent import MultiAgentAccelPOEnv
from flow.networks import FigureEightNetwork
from flow.core.params import TrafficLightParams  # ì‹ í˜¸ë“± ì„¤ì • ì¶”ê°€

# ğŸš€ RL í•™ìŠµ íŒŒë¼ë¯¸í„° ì„¤ì •
HORIZON = 1500  # í•œ ë²ˆì˜ rolloutì´ ì§€ì†ë˜ëŠ” ì‹œê°„ (step)
N_ROLLOUTS = 20  # í•œ ë²ˆì˜ í•™ìŠµ iteration ë™ì•ˆ ì‹¤í–‰ë˜ëŠ” rollout ê°œìˆ˜
N_CPUS = 2  # ë³‘ë ¬ ì‹¤í–‰í•  ì‘ì—…ì ìˆ˜

TARGET_VELOCITY = 20  # ëª©í‘œ ì†ë„ (m/s)
MAX_ACCEL = 3  # RL ì°¨ëŸ‰ ìµœëŒ€ ê°€ì†ë„
MAX_DECEL = 3  # RL ì°¨ëŸ‰ ìµœëŒ€ ê°ì†ë„

NUM_AUTOMATED = 1  # RL ì°¨ëŸ‰ ê°œìˆ˜ (1, 2, 7, 14 ì¤‘ ì„ íƒ ê°€ëŠ¥)
assert NUM_AUTOMATED in [1, 2, 7, 14], "NUM_AUTOMATED ê°’ì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."

# ğŸš€ Ray ì´ˆê¸°í™” (ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€)
ray.init(ignore_reinit_error=True)

vehicles = VehicleParams()

# RL ì°¨ëŸ‰ ì¶”ê°€ (NUM_AUTOMATED ë§Œí¼ ì¶”ê°€)
for i in range(NUM_AUTOMATED):
    vehicles.add(
        veh_id=f"rl_{i}",
        acceleration_controller=(RLController, {}),
        lane_change_controller=(StaticLaneChanger, {}),
        routing_controller=(ContinuousRouter, {}),
        car_following_params=SumoCarFollowingParams(
            speed_mode=31,
            accel=MAX_ACCEL,
            decel=MAX_DECEL,
        ),
        num_vehicles=1
    )

# ê¸°ì¡´ IDM ì°¨ëŸ‰ ì¶”ê°€
vehicles.add(
    veh_id="idm",
    acceleration_controller=(IDMController, {}),
    lane_change_controller=(StaticLaneChanger, {}),
    routing_controller=(ContinuousRouter, {}),
    car_following_params=SumoCarFollowingParams(
        speed_mode=0,
        decel=2.5,
    ),
    initial_speed=0,
    num_vehicles=13
)

# ì‹ í˜¸ë“± ì„¤ì • ì¶”ê°€
traffic_lights = TrafficLightParams(baseline=False)
traffic_lights.add(
    node_id="center",
    tls_type="static",
    programID="1",
    phases=[
        {"duration": "10", "state": "GrGr"},
        {"duration": "3", "state": "yrGr"},
        {"duration": "2", "state": "rrrr"},
        {"duration": "10", "state": "rGrG"},
        {"duration": "3", "state": "ryrG"},
        {"duration": "2", "state": "rrrr"}
    ]
)

flow_params = dict(
    exp_tag='figure8_with_rl',
    env_name=MultiAgentAccelPOEnv,
    network=FigureEightNetwork,
    simulator='traci',
    sim=SumoParams(render=True, sim_step=0.1),
    env=EnvParams(
        horizon=HORIZON,
        additional_params={
            "target_velocity": TARGET_VELOCITY,
            "max_accel": MAX_ACCEL,
            "max_decel": MAX_DECEL,
            "sort_vehicles": False
        },
    ),
    net=NetParams(additional_params=ADDITIONAL_NET_PARAMS.copy()),
    veh=vehicles,
    tls=traffic_lights
)

# í™˜ê²½ì´ ì´ë¯¸ ë“±ë¡ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ í›„ ë“±ë¡
env_id = "MultiAgentAccelPOEnv-v0"
if env_id not in gym.envs.registry.env_specs:
    create_env, env_name = make_create_env(params=flow_params, version=0)
    register_env(env_name, create_env)

# ğŸš€ PPOTrainerë¡œ RL í•™ìŠµ ì‹¤í–‰
trainer = PPOTrainer(env=env_name, config={
    "num_workers": N_CPUS,
    "train_batch_size": HORIZON * N_ROLLOUTS,
    "sgd_minibatch_size": 256,
    "num_sgd_iter": 10,
})

# RL í•™ìŠµ 10íšŒ ë°˜ë³µ
for i in range(10):
    result = trainer.train()
    mean_reward = result.get("episode_reward_mean", 0)  # ì—ëŸ¬ ë°©ì§€
    print(f"Iteration {i}, reward: {mean_reward}")

# í•™ìŠµëœ ì •ì±… ì €ì¥
checkpoint_path = trainer.save()
print(f"Checkpoint saved at {checkpoint_path}")

print(f"Traffic light parameters: {flow_params['tls']}")
print("Flow parameters:", flow_params)
